


Most security solutions are implemented into existing systems with their own task dependencies and responsibilities. For example, the primary purpose of a job application service website is to manage resumes, cover letters and communications between hiring managers and job candidates. However, since anyone in the world can submit PDFs and other documents to HR departments using this service, this could pose as an easy entry vector for malicious actors targeting specific companies. Therefore, the best place to implement malware scanning solutions is to work directly in-line with processes such as this. A well deployed malware solution should only be noticed once a detection has been made, and not hinder the systems it has been integrated with. In-line scanning works well, as it can be deployed at various layers of segmentation within an internal network, and levels of file and program scrutiny can be tuned and tightened accordingly. Remember, the types of malware techniques we deploy are heavily dependent on the type of data we expect, and the volume in question. With this in mind, let’s look at some other techniques we can use with an in-line security approach to protect our users and defend against all manner of attacks. Click the Static Analysis icon to learn about the first. You will be able to proceed after viewing them all.Concept of MultiscanningIn the effort to track down a criminal, a single top-notch detective may scout for clues and scour a crime scene to spot anything out of place. There’s a chance the culprit will be identified; there’s a chance something will be missed, and the perpetrator gets away to continue their crime spree. Bring on a second skilled detective, and the collaboration will likely have greater success in identifying and apprehending the criminal. Bring on a whole team of industry-leading experts, and the criminal will have nowhere to hide. The perpetrator is captured and put behind bars, where they no longer pose a threat to the public.For example, between June 29 and July 29 of 2019, the use of four antimalware engines in conjunction resulted in a 68% detection rate of the most searched threats on MetaDefender Cloud in that period. With eight scanners, the detection rate climbed to 81.4%. A 99% detection rate was not achieved until 20 engines were deployed. Malicious software, more commonly referred to as malware, is like a web of digital criminals. Viruses, spyware, trojans, and worms, are constant threats that have consequences for you from minor annoyances to tens or hundreds of thousands of dollars paid to a ransomware attackWhen a cybercriminal releases a new strand of malware that hasn’t been seen before, AV engines haven’t had a chance to learn about it, which is why it's referred to as a zero day threat. Antimalware companies are first responders, trying to catalogue new discoveries as quickly as possible, which makes their engines more robust. With estimates of 350,000 new malicious codes released daily, even the best detective will benefit from team collaboration. While all do excellent jobs, some, like our detectives, will spot clues that others might miss. Sometimes it will be the lesser-known vendor that is the first to add the detection to their engine. File-Based Vulnerability AssessmentSoftware is a product made by people and therefore, is inherently imperfect. Imperfections that allow malicious actors to hack your system are referred to as vulnerabilities. While working with vulnerabilities is common, it becomes alarming when software critical to your business has a vulnerability that is exploited.Document creation tools, video players, web browsers, and Operating Systems - any one of them may have vulnerabilities that cybercriminals can utilize to put your privacy or proprietary data at risk.Many are surprised to discover how many vulnerable applications are running on their business and personal computers, printers, mobile devices, and other endpoints. Vulnerabilities will always exist but identifying where they are, and their associated degree of risk can help IT administrators find ways to safely utilize vulnerable app’s within their organizations.Let’s walk through a common example. According to Oracle, 97% of enterprise desktops run Java. Java releases regular updates that patch vulnerabilities that the company-and others-discover in the software. In our example, Larry X’s out these annoying notifications, failing to download patch after patch of critical security updates. His risk to be exploited grows each day.Separately, hackers build tools that infect websites and lay in wait for vulnerable endpoints. The tools automatically scan devices accessing the compromised site, detect the ones that that have a vulnerability, and spring an attack, bypassing Java’s security and dropping a payload to the system. Because Larry hasn’t updated his software with security patches, his computer is sniffed out and breached. All his passwords, confidential company information, bank accounts, and even personal data are now in the hands of a criminal.At a different company, the files on Elizabeth’s computer have just been scanned by OPSWAT’s Vulnerability Detection Engine. The Java vulnerability was immediately detected. Elizabeth was presented with steps for remediation, which included options to update or disable applications. To help guide her decisions, this information was presented as a list of CVE, or Common Vulnerabilities and Exposures. All the vulnerabilities had a score based on an industry standard called the Common Vulnerability Scoring System, or CVSS. Scores are calculated based on exploit susceptibility and the impact it would have.Understanding your system vulnerabilities is critical to your organization’s security. It empowers end users and IT administrators to avoid potentially dangerous pitfalls or to find suitable workarounds to keep your business safely running.Dynamic Analysis RecapA suspicious package constitutes an unknown variable. It could be something as welcome as a surprise gift box from a friend, but it might also be something unwanted, or even harmful.  Now imagine having a room that is somehow separate from the world, where consequences never become reality. You could shake the box, open it, and noodle around with anything you find inside. If it’s something you want, great-take it with you back into the real world. If it’s something dangerous, then no matter how bad or powerful it is, you can keep it quarantined in the room forever-it can never escape into the real world. Sandboxing provides a digital version of that construct. It’s a security technology that allows any file to perform the actions it’s designed to - a spreadsheet can compute formulas, and a dot exe file can launch a program installation-all in a controlled and isolated environment separated from real-life consequences. The power of this technology is that it allows dynamic analysis of the file in action. It uncovers elements that might not be noticed-or even activated-unless and until a static file has been put in motion. Each activity that is observed when a file runs in the sandbox is monitored, analyzed, and assigned points on a scale. When all processing is completed, the various points are aggregated to a rating, typically expressed as a confidence score that indicates how likely it is that a given file is malicious. The lower the score, the lower the risk. Let’s say we have a seemingly innocuous word document that got flagged during a scan. We opt to put it in the sandbox for analysis. It immediately gets 11 points, because that’s a standard risk for a document file. The scan further discovers the document has white text on a white background, which means someone may be trying to hide something. The sandbox tools click through the invisible text and a hidden link activates ransomware-a pop up tells us our files are encrypted, and it will cost us $20,000 to get the key to unlock our data. No need to add up our points-we know this file can infect our entire system. We exit the sandbox and return back to reality but leave the malware and all its risks encapsulated behind us. Other results will not be as dramatic. Files put into the sandbox are given a score from 1 - 100%. The end user can be very confident that a rating of 5% means a file is clean; they can be equally confident that an 85% rating means a file should be quarantined. In any case, the confidence score and examination of observed activities can be used for forensic analysis, threat assessment, and other research.DLPLet’s talk about data loss. It seems that hardly a week can go by without hearing news of yet another major company falling victim to a breach. For every headline, there are scores of other companies that unintentionally-or even unknowingly--handed off their treasure trove of data to criminals.We’ve reached an era in which storing and transferring information is easier than ever before. For convenience and to beat deadlines, we might email PDFs of tax forms, share spreadsheets with team members’ names and birth dates, or send documents containing credit card information. While financial institutions, medical facilities, government agencies, and similar industries are obvious targets for data breaches, the truth is that any company that transmit's sensitive information is a goldmine for identity thieves and other cyber criminals. Keeping your data within your organization is now more important than ever.Most Data Loss solutions focus on detecting and blocking mechanisms of financial or personally identifiable information. In other words, if the software identifies patterns that look like social security numbers, credit cards, and similar private information, it suppresses sensitive information with automatic document redaction, metadata removal, or watermark addition.CDRDistillation is a basic procedure - a body of water is heated until it becomes steam. The steam travels through a tube and then collects and cools in another container, where it returns to its liquid state. You could do exhaustive research to find out which threats you avoided - lead, bacteria, viruses, and more, or you could simply enjoy knowing your water is clean enough to drink.Content Disarm and Reconstruction, or CDR, employs a similar process. To continue the analogy, in cyber security, our body of water is comprised of individual files, including complex file types which are particularly vulnerable to potentially malicious exploitation. With their grouped and nested components, they can be containers of malware, ready to pour out security risks the moment an associated program acts upon the file. Cybercriminals regularly exploit the necessary code execution on commonly used complex files to compromise a system. With surgical precision however, CDR breaks the file to it's smallest components and removes any and every potential threat.Like distillation, CDR doesn’t have any prejudice about identifying which contaminants are in a potential threat vector-it just eliminates them completely, and then reconstructs all the remaining pieces back into a safe and fully functional file.For example, CDR will scrub away hidden files or messages maliciously embedded within a digital photo, but the final disarmed picture looks and behaves exactly as a safe image should. When a file begins the CDR process, we don’t know if it’s contaminated; we just know the file is clean and working when the process is done. Microsoft Office files and PDFs are the most common file types used in the first stage of social engineering attacks. Because they can contain and execute code, these files frequently install backdoors, ransomware, bots, Advanced Persistent Threats (APTs), and more. The CDR process also flattens, neutralizes, and reconstructs files such as these.OPSWAT takes standard CDR technologies even further with Deep CDR, differentiated by its recursive approach. To explain the difference, say you create a desktop folder and label it “Budget”. This is the capstone of a pyramid we’re about to construct. In that folder, we’re going to add a spreadsheet labeled “Overview”, and another folder labeled “Current Year”.These two constitute the next level of our pyramid. In the Current Year folder, we add folders for Q1, Q2, Q3, and Q4-our next level down in the pyramid. Each folder gets multiple files that outline forecasts and budgets for that quarter. We can add layers indefinitely, until we have built a huge pyramid with a complex structure.If this directory were zipped into a single archive and submitted for standard CDR, it would disarm and reconstruct only the capstone-nothing more. If we want to send our second layer through the process, we must manually submit those individual components and then again, only that single layer is scrubbed. While effective, this process is not efficient. Recursion, on the other hand, works its way through layers until reaching the last. Deep CDR, in it's recursive approach, would sanitize our capstone and recognize another layer is underneath. Automatically, it drops to the second level, moving laterally until that level is fully sanitized.Without need for any further user action, the process continues in a down and lateral movement, layer after layer, until the entire pyramid has been fully disarmed and perfectly reconstructed.Because of its process, CDR can only be performed on supported file types, as there needs to be a known way to reconstruct a file so that it is both safe and functional. OPSWAT’s list of supported file types for Deep CDR is continuously expanding to cover as many common use cases as possible.Importantly, CDR and Deep CDR are not intended as detection tools. It is OPSWAT solution to address any class of Malware that we can consider an unknown threat. Its success is not measured by giving you detailed diagnostics of the contaminants left behind in the processes; but achieved through knowing your file is safe. Big Data AnalysisIn the digital age, the arms race is access to data. When we gather large amounts of data, it allows us to see things we otherwise wouldn't. What the traffic will be like, the upcoming weather, or even how best to sell a product. However, the logic of having more data to make better decisions isn't enough, as having huge amounts of data on its own can create noise and add extraneous points of information. Aggregating data isn't meaningful if it can't be structured and analyzed. With proper analysis, data is transformed into information. This information can then be used to decide a course of action to change or take advantage of the emerging patterns.We can consider these evaluation strategies of analyzing information as intelligence. Using intelligence to analyze data is especially important when taking action to protect your technological assets. Effectively and intelligently analyzing patterns of malicious content is paramount to preventing outbreaks or stopping them in their tracks.
Types of in-line scanning 
1. Multi-scaning
2.Vulnarability Assessmnent
3. Dynamic Analysis
4 DLP
5. CDR
6. BIG data analysis
Software imperfections that allow malicious actors to hack systems are vulnerabilities
probability tools is used to interprete dynamics analysis result 


By now we are familiar with the concept that no single security solution can cover all the aspects of securing our critical networks. The best approach is to use several solutions to cover many aspects of security in critical network data flows. Security companies know this, and they strive to create methods of integration not only with the flow of data, but also with other security solutions. When multiple security solutions are used for a single data entry point or logical data workflow process, it’s referred to as a security stack.Most security implementations follow a hand-off approach, where data is provided to a system, usually for scanning or file alteration, then provided back to the workflow. Data is presented in it's raw, copied, or adapted form, and then sent to another system to process the information and provide a result.Sometimes in-line data might be part of a security solution, but the notification mechanisms that occur upon detection follow a more retroactive approach. For example, file data might be sent to a sandbox as an auxiliary or secondary scanning tool, where waiting for files doesn’t make sense for the workflow. In these instances, systems need to be able to programmatically track and notify proper parties and systems after the secondary detection.The question becomes, how do security systems interact with existing data flows, and with each other? Luckily, there are several standardized methods of enabling data hand-off. Out-Of-The-BoxSeveral security software companies develop a large breadth of technologies, tools, deployable appliances, and solutions. These companies have various products that, because they are in-house, are designed to work seamlessly with one another and to connect with the push of a button, or other simple connections. The actual technical backend of out-of-the-box integrations can vary, but the point is for security administrators who purchase the products to have a solution that just works. Additionally, some vendors have very strong technical partnerships and work closely with each other to create seamless integrations with another companies’ solution. Out-of-the-box integrations are the most difficult to create, but when done properly are preferable to hiring many cyber security professionals to do the same job. Standardized ProtocolsAs Hypertext Transfer Protocol (HTTP) and Simple Mail Transfer Protocol (SMTP) [see glossary for more information on HTTP, SMTP, HTTPS, API, and ICAP] are standardized protocols for web traffic and email transfer respectively, there are several protocols created to specifically enable easy security hand-offs for scanning or other processes. A great example for this is ICAP, or Internet Content Adaption Protocol. ICAP is a standardized, lightweight protocol designed to offload cybersecurity tasks onto specialized servers to evaluate network throughput. This is a method of sidebar communication between network appliances. What makes ICAP extremely useful is most web proxy solutions can communicate ICAP, and therefore easily hand-off network traffic for scanning. Many cyber security vendors leverage standardized protocols such as this to allow for seamless integration into other systems. APIAPI, or Application Programming Interface, is a programmatic interface that allows software systems to communicate with other software systems. APIs only expose necessary information, actions, and specified parts of a system, thus preventing exposure of the interworking of the software code from connecting systems. A perhaps overly simplified but useful analogy is thinking of a drive-through window at a fast-food restaurant. You are presented a list of possible options, and you make your request. You don’t watch the food being prepared, but your order will be provided to you at the window regardless. If you make a request out of the scope of the menu, the restaurant will not comply. And if you make too many orders per second, you’re probably not going to get very far. You need to make your order within the parameters of the drive-through.API’s for many systems have a list of available calls that can be requested. Security systems use APIs to send and receive all kinds of information and can be used to integrate with other solutions built in-house, by establishing a programmatic connection.  WebhookIn our example of a secondary sandbox solution that retroactively sends a detection notification, the information sends to additional security systems, or for administrative notifications, often occurs in the form of a Webhook. Webhooks send data, usually over HTTP or Hypertext Transfer Protocol Secure (HTTPS) trigger events. In the case of the sandbox detection when the file finished the dynamic scan and was flagged as malicious. Webhooks are not used explicitly for retroactive actions and can easily be used as a real-time communication method with in-line solutions. With cyber security solutions, they function as an excellent method to trigger reactive detection response event procedures.

While we always want to cover all areas of vulnerably, there are potential pitfalls when dealing with implementing large security stacks into our data workflows. Increased security can often come at the cost of increased complexity, and the more moving pieces there are, the higher the likelihood of a potential failure. Therefore, it’s imperative that when any new solution is introduced to a secure environments, proper testing and validation is completed before moving to a production environment. Ideally, the various solutions will have a common compatibility certificate, as seen with various vendors and their technical alliances.\n\nAs with any other proper implemented critical IT solutions, high availably, scaling, and contingency plans are all important when architecting a security solution or stack, if a single system goes down, consider how that effects the other systems in an overall workflow, where the bottlenecks are, and ultimately how to minimize downtime in the event of a downed system.\n\nWith proper analysis and planning, you can run a secure environment, with excellent uptime